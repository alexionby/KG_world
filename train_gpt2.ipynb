{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e553649-52fe-4f0c-b2f2-c58c274ad07b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ToDo:\n",
    "1. shuffle before train\n",
    "2. Do something with '\\n' and statement length\n",
    "3. Fix trouble with dot ., make generated sentences separable, add <eos_token> or another sep\n",
    "4. ToDo: write custom beam search over model outputs, and vary length, then check whether generated names of `entities` have sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982722a0-01b2-4f4e-a5b6-8f45d733fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from IPython.display import Pretty\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ed0a9e-75a5-4dc6-98f9-12f58f52d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5445d8b1-47f9-41b5-94f4-63ba0c86767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a0eddd4-3460-4af7-9c11-87fce694e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"statements.txt\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab49962-2575-47a7-af2f-fa3349597c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6e7dd70-27dd-420e-bdec-ac7186d73727",
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = data.split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55af5a95-1f99-477d-a673-cb1539e75d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_part = []\n",
    "right_part = []\n",
    "\n",
    "for statement in statements:\n",
    "    statement = statement.split(' | ')\n",
    "    random.shuffle(statement)\n",
    "    lp = statement[:-1]\n",
    "    rp = statement[-1:]\n",
    "    left_part.extend(lp)\n",
    "    right_part.extend(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1dec61fb-da28-4f2e-99c2-768b6c9bdb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39324, 24826)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(left_part), len(right_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11aef66d-9b6d-481c-9f7e-d8f4d4aadc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_part = [s.split(' | ')[0] + '\\n' for s in statements]\n",
    "# right_part = [s.split(' | ')[1] + '\\n' for s in statements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fddf84ac-68e7-4ad3-a884-9df95721efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(left_part)\n",
    "random.shuffle(right_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dac5806-db0c-438f-8755-ce63348dbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_part = \". \".join(left_part)\n",
    "right_part = \". \".join(right_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15cf17-1013-4b7e-a40d-b5a02ed9190c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09dd3bd5-57e4-4859-8eb0-74db9c34d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_dir = \"temp_train_txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "285e4452-6090-4713-bd68-367edebf151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(temp_data_dir):\n",
    "    shutil.rmtree(temp_data_dir)\n",
    "    \n",
    "os.makedirs(f\"{temp_data_dir}\", exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "901a8fdb-d3c3-48c6-a848-ecd24f5b86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{temp_data_dir}/train.txt\", \"w\") as f:\n",
    "    f.writelines(left_part)\n",
    "    \n",
    "with open(f\"{temp_data_dir}/test.txt\", \"w\") as f:\n",
    "    f.writelines(right_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "775a570d-32f5-41a5-b4bb-4d5d1023b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d107757a-4ce3-4091-9eda-f3c49a573561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(\"temp_files/gpt2-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31fef297-9ee8-4d4c-8e04-03ef6f2127d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "train_path = f\"{temp_data_dir}/train.txt\"\n",
    "test_path = f\"{temp_data_dir}/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3158c709-37e4-4ab8-8968-5abd1095f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads cached tokenized text from `temp_train_txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14d322d3-500e-4922-9391-4721eeac6a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (488404 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128, )\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6670bed9-6867-4591-8831-4f758d29251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eclipsix Eclipse Inc. conducts its business in EVE. Gusev Government shares border with Korolev Kingdom. Hellas Hierarchy shares border with Daedalia Democracy. Nebula Nuances is the official language of Wahhabi Ward. Orbit Optimizations Org operates in Assembly of Ara. Planetary Platinum operates in GRD. HyperSpace conducts its business in WTL. Noon Nuance is the official language of TMB. Lunar Locale is the capital of Delta. Vortexdyne has a presence in Hierarchy of Hebes. Interstellar Indium Corp. conducts its business in STM. Zephyros has a presence in\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Fluxara Starveil worked in ACML. Lab for Quantum Battle Logistics is a research partner of Earthbound Enterprises Inc.. NebulaNexus operates in Chasma. Aquila AI Corp. operates in Radau Regime. Argyre Authority maintains diplomatic relations with Ismenius Imperial. Spherigon collaborates with Center for Tunable Lasers. Galaxy Glyph is the official language of Monarchy of Mangrove. SolarSilica Co. operates in Ophir Order. Asteroid Analytics has a presence in Yankee. Planetary Pidgin is taught as a second language in Flora. Astrolian Alliance\n"
     ]
    }
   ],
   "source": [
    "for tr,ts in zip(train_dataset, test_dataset):\n",
    "    print(tokenizer.decode(tr))\n",
    "    print(\"|\" * 100)\n",
    "    print(tokenizer.decode(ts))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a97a0-4bab-41e6-bf9a-271e84b388fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fee0b24-3a7b-4dd7-9e17-f1d8c97221d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9683c81-5eb5-4475-ab6f-f87c6822e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    ignore_index = tokenizer.pad_token_id\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), labels.view(-1), ignore_index=ignore_index)\n",
    "    return {'perplexity': torch.exp(loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50016634-9f0c-4fd8-b1ab-b5ccf92656ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d903d9f6-0220-4167-8525-782060ed6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"temp_files/gpt2-trainer\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=20, # number of training epochs\n",
    "    per_device_train_batch_size=16, # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=100,\n",
    "    eval_steps = 100, # Number of update steps between two evaluations.\n",
    "    save_steps=500, # after # steps model is saved \n",
    "    warmup_steps=100,# number of warmup steps for learning rate scheduler\n",
    "    gradient_accumulation_steps=2,\n",
    "    # gradient_checkpointing= ???\n",
    "    # prediction_loss_only=True,\n",
    "    # eval_accumulation_steps=32,\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    # preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c274b-4742-487f-ab11-1ee4f6a0a9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b290371-7c29-42d9-bf6c-931521b71d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\sd_xformers\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexionon\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Projects\\KG_world\\wandb\\run-20230714_153907-hu1bt65a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexionon/huggingface/runs/hu1bt65a' target=\"_blank\">likely-fire-29</a></strong> to <a href='https://wandb.ai/alexionon/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexionon/huggingface' target=\"_blank\">https://wandb.ai/alexionon/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexionon/huggingface/runs/hu1bt65a' target=\"_blank\">https://wandb.ai/alexionon/huggingface/runs/hu1bt65a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2380' max='2380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2380/2380 28:46, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.441300</td>\n",
       "      <td>2.006718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.832500</td>\n",
       "      <td>1.620961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.611800</td>\n",
       "      <td>1.504718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.516900</td>\n",
       "      <td>1.433243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.456700</td>\n",
       "      <td>1.392714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.413300</td>\n",
       "      <td>1.364136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.376200</td>\n",
       "      <td>1.341173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.349200</td>\n",
       "      <td>1.316834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.318900</td>\n",
       "      <td>1.295460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.284000</td>\n",
       "      <td>1.265108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.248200</td>\n",
       "      <td>1.237368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.219300</td>\n",
       "      <td>1.220908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.190400</td>\n",
       "      <td>1.205775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.169900</td>\n",
       "      <td>1.198511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.150400</td>\n",
       "      <td>1.192494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.140300</td>\n",
       "      <td>1.188794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.124500</td>\n",
       "      <td>1.183245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.115200</td>\n",
       "      <td>1.176098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.103800</td>\n",
       "      <td>1.171953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.094800</td>\n",
       "      <td>1.169091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.085300</td>\n",
       "      <td>1.169220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.081700</td>\n",
       "      <td>1.168863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.077800</td>\n",
       "      <td>1.166463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2380, training_loss=1.3555653003083559, metrics={'train_runtime': 1734.0808, 'train_samples_per_second': 44.0, 'train_steps_per_second': 1.372, 'total_flos': 4963830054912000.0, 'train_loss': 1.3555653003083559, 'epoch': 19.92})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb0a3987-7616-44d6-af0d-1e85e0274b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf547f6-2b53-4ddc-a11a-133aa4212d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4bf05796-dd17-4a5f-8c32-f1bcaf1334aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "kg_world = pipeline('text-generation', model='./temp_files/gpt2-trainer', tokenizer=MODEL, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fbea4d74-7f42-48d7-93a7-9036bb9a229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   37, 22564,  3301,  2907,   303,   346,  3111,   287,  7125,  5805,\n",
      "           13,  3498,   329, 29082,  5838,  5972,  3969,   318,   257,  2267,\n",
      "         5212,   286,  3668,  7784, 41253,  3457,   492, 46915,    45,  1069,\n",
      "          385, 14051,   287,   609, 11797,    13, 11446, 10102,  9552, 11421,\n",
      "           13, 14051,   287,  5325,   559,  3310,   524,    13,   943,  1360,\n",
      "          260, 11416, 16047, 13093,  2316,   351,  1148,  3653,  3754, 11773,\n",
      "           13,  1338,   372, 37107,  6967,   689,   351,  3337,   329, 13932,\n",
      "          540, 10123,   364,    13,  9252, 27949,   746,   318,   262,  1743,\n",
      "         3303,   286,  2892,  9282,   286, 27609,   305,   303,    13, 12347,\n",
      "        15086,  3970,  1766,    13, 14051,   287,   440,   746,   343,  8284,\n",
      "           13, 38484,  1868, 30437,   468,   257,  4931,   287, 36514,    13,\n",
      "        43800,   350,   312,  1655,   318,  7817,   355,   257,  1218,  3303,\n",
      "          287,  4432,    64,    13,  8304,  3225,   666, 10302])\n"
     ]
    }
   ],
   "source": [
    "for sample in test_dataset:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a907b29-a8c8-4743-b8bf-1592b9767ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = tokenizer.decode(sample).split(\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "904080b2-fd2b-4a52-878d-54659fc8bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = random.choice(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "386d0d71-a414-44ba-988c-4540a4946814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fluxara Starveil worked in ACML'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c8dcf25c-f817-4833-9abf-8a8c44f236fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fluxara Starveil worked in ACML. Stellaris Security operates in YAN. The headquarter of Alien Aggregate is in Surveillance Suburb. Martian Transport Services LLC collaborates with IALSS. Exoterra Solutions Ltd. established NI27032MOO. Quantum Quorums Corp conducts its business in DEI. Stellar Stream Services Corp. has a presence in Coprates Confederacy. The capital of Assembly of Ares is Zephyr-Zone-0. The native language of Order of Ophir is Ares Articulate. Horizon Nanotech has its central office located in Phobos Port. SolarSilica Co"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = kg_world(test_sentence)\n",
    "generated_text = output[0]['generated_text']\n",
    "Pretty(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "db662893-233f-458e-a14b-f04802c116f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[O] Fluxara Starveil worked in ACML\n",
      "[O] Interstellar Ironworks Co\n",
      "[-] started LU76690GLO\n",
      "[X] Asteroid Antimony has a presence in Radau Regime\n",
      "[O] Horizon AI Solutions LLC has a presence in Jasmine Jurisdiction\n",
      "[X] Horizon AI Solutions LLC operates in Lily League\n",
      "[X] Solar Smelters operates in Hephaestus\n",
      "[O] Martian Artifacts Preservation Ltd\n",
      "[O] conducts its business in Principality of Phobian\n",
      "[-] Company Martian Transport operates within the realm of Astrophysics Research\n",
      "[O] Vexta Co\n",
      "[O] operates in Utopia Union\n",
      "[X] Pulsar Power has a presence in Radau\n",
      "[O] The capital of Th\n"
     ]
    }
   ],
   "source": [
    "for sample in generated_text.split(\". \"):\n",
    "    if sample in right_part:\n",
    "        print(\"[O]\", sample)\n",
    "    elif sample in left_part:\n",
    "        print(\"[X]\", sample)\n",
    "    else:\n",
    "        print(\"[-]\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0362b29d-44c8-4a13-b5bd-e0979a31de8f",
   "metadata": {},
   "source": [
    "### Returning token proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76998f03-33d5-4473-b66d-fff07ad53578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14d5264c-cfd0-4b35-95ad-1cc714e8246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL)\n",
    "model = GPT2LMHeadModel.from_pretrained('./temp_files/gpt2-trainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42481dbc-ced8-4c84-af8b-832ae09b6e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89b0883-32d6-4626-8a33-8af1df8d0455",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18663852-ec9e-4291-97c3-1ad6cd7af77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=128, \n",
    "                                     pad_token_id=tokenizer.pad_token_id, \n",
    "                                     do_sample=False, \n",
    "                                     num_beams=5, \n",
    "                                     num_return_sequences=1, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30c254aa-2153-4741-931f-79b42f8eb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Horizon AI Solutions LLC has a presence in Jasmine Jurisdiction\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model.generate(encoded_input['input_ids'], generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83578d43-f3d5-44b7-a33e-8ed5d4b1bd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon AI Solutions LLC has a presence in Jasmine Jurisdiction. The primary language of communication in Xezex Xerocracy is Oxia Oratory. The educational curriculum of Xylophyle Xerocracy includes learning Oxia Oromo. The educational curriculum of Xezex Xerocracy includes learning Oxia Oromo. The educational curriculum of Xezex Xerocracy includes learning Oxia Oromo. The educational curriculum of Xylophyle Xerocracy includes learning Xanthe Xhosa. The educational curriculum of Xezex Xerocracy includes learning Oxia Oromo. The educational curriculum of Xezex Xerocracy includes learning Oxia Oromo. The educational curriculum of X\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
     ]
    }
   ],
   "source": [
    "for sidx in range(len(output)):\n",
    "    decoded_output = tokenizer.decode(output[sidx], skip_special_tokens=True)\n",
    "    print(decoded_output)\n",
    "    print(\"|\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a46ed0d-69e2-473a-a8a8-43ea2c533e62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'right_part' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mprint\u001b[39m(rp[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m rp \u001b[38;5;129;01min\u001b[39;00m \u001b[43mright_part\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrbit Ore Organics Inc.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rp]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'right_part' is not defined"
     ]
    }
   ],
   "source": [
    "_ = [print(rp[:-1]) for rp in right_part if \"Orbit Ore Organics Inc.\" in rp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144704e-48ea-4ae7-93e3-248acce65e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(encoded_input['input_ids'])\n",
    "encoded_input['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa939b7-f795-4c9d-beb7-69c2ac7deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    res = model(encoded_input['input_ids'])\n",
    "    encoded_input['input_ids'][0]\n",
    "\n",
    "    for idx, (token, token_idx) in enumerate(zip(res.logits[0], encoded_input['input_ids'][0]), start=1):\n",
    "        # convert to probabilities (softmax function)\n",
    "        probabilities = torch.nn.functional.softmax(token, dim=-1)\n",
    "\n",
    "        # pick the token with the highest probability or sample from the distribution\n",
    "        # next_token = torch.argmax(probabilities, dim=-1)\n",
    "        _, next_token = torch.topk(probabilities, 5, dim=-1)\n",
    "        # next_token = torch.multinomial(probabilities, num_samples=10)\n",
    "\n",
    "        # decode it back to a token\n",
    "        decoded_token = [tokenizer.decode(t) for t in next_token]\n",
    "\n",
    "        print(tokenizer.decode(encoded_input['input_ids'][0][:idx]), \"---\", decoded_token)\n",
    "        print(tuple(zip(decoded_token, probabilities[next_token].cpu().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc1220-5d77-4cad-8d2c-c772e245c8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d785f53-ebdb-4f69-9ca0-945d51eee881",
   "metadata": {},
   "source": [
    "ToDo: write custom beam search over model outputs, and vary length, then check whether generated names of `entities` have sens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4af18-e5a8-464b-824f-c657be001f66",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c691906-43c8-4668-9ac6-f51e64ac76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "562702b8-a111-49be-9cc6-3e5c797edf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "27798129-801b-471e-b8dd-a752f78438fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('./temp_files/gpt2-trainer')\n",
    "context = tokenizer.encode(\"Horizon AI Solutions LLC has a presence in\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e8ad71f1-e297-469a-8cd9-ad24f86c0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp_train_txt/test.txt\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "84b41d5c-d797-4c54-ae8e-b17c546f20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b9935b35-7993-415c-9d49-d89126ec49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_v2(model, length, context, beams=5):\n",
    "    # context = context.to(device)\n",
    "    generated = [context for _ in range(beams)]  # Initialize the list of hypotheses\n",
    "    beam_scores = torch.zeros((beams, ), dtype=torch.float, device=device)\n",
    "\n",
    "    for _ in tqdm(range(length)):\n",
    "        all_candidates = []  # Store candidates at each step\n",
    "        hypothesis_set = []\n",
    "        \n",
    "        for idx, hypothesis in enumerate(generated):\n",
    "            with torch.no_grad():\n",
    "                predictions = model(hypothesis)\n",
    "            predictions = predictions['logits']\n",
    "            predictions = predictions[:, -1, :]  # Take the prediction of the last token\n",
    "            scores = torch.nn.functional.log_softmax(predictions, dim=-1)  # Convert logits to log probabilities\n",
    "\n",
    "            # Add the total score\n",
    "            scores = scores + beam_scores[idx]\n",
    "            scores = scores.view(-1)  # Reshape the scores to a single dimension\n",
    "\n",
    "            # Get the top k scores and ids\n",
    "            best_scores, best_scores_id = torch.topk(input=scores, k=beams, dim=-1, largest=True, sorted=True)\n",
    "            # print(best_scores, best_scores_id)\n",
    "\n",
    "            # Get the corresponding tokens and update the hypotheses\n",
    "            best_scores_id = best_scores_id % model.config.vocab_size\n",
    "            for i in range(beams):\n",
    "                # print(hypothesis.shape, best_scores_id.shape)\n",
    "                new_hypothesis = torch.cat((hypothesis[0], best_scores_id[i].unsqueeze(0)), dim=0)\n",
    "                new_hypothesis = torch.unsqueeze(new_hypothesis, 0)\n",
    "                \n",
    "                nh = new_hypothesis[0].numpy().tolist()\n",
    "                if nh in hypothesis_set:\n",
    "                    continue\n",
    "                else:\n",
    "                    hypothesis_set.append(nh)\n",
    "                    all_candidates.append((new_hypothesis, best_scores[i]))\n",
    "        \n",
    "        # print(all_candidates)\n",
    "        # Order all candidates by score and select top beams\n",
    "        ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "        generated = [x[0] for x in ordered[:beams]]\n",
    "        beam_scores = torch.Tensor([x[1] for x in ordered[:beams]])\n",
    "        print(beam_scores[:5])\n",
    "\n",
    "    return generated, hypothesis_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bc24240c-38e8-4e1f-b2ef-cce55a9b11b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cc83b5bf4d42f8b11884bb1eba0ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4761, -2.3040, -2.3112, -2.7588, -2.8299])\n",
      "tensor([-3.1506, -3.9192, -3.9626, -3.9949, -4.2216])\n",
      "tensor([-3.8680, -3.9542, -3.9631, -4.0054, -4.2218])\n",
      "tensor([-3.9322, -3.9662, -4.2797, -6.8473, -7.1719])\n",
      "tensor([-3.9325, -4.2797, -6.7965, -6.8344, -6.8634])\n",
      "tensor([-3.9325, -4.2797, -7.9765, -8.1920, -8.2395])\n",
      "tensor([-4.4775, -6.0360, -6.0805, -6.4265, -6.5252])\n",
      "tensor([-6.1771, -6.6564, -6.7002, -6.8581, -7.1190])\n",
      "tensor([-6.1771, -6.6573, -6.7002, -8.4082, -8.4153])\n",
      "tensor([-6.7024, -6.8785, -6.8813, -8.4082, -8.4153])\n",
      "tensor([-6.8786, -6.8814, -8.4153, -9.4680, -9.9694])\n",
      "tensor([ -9.1989,  -9.3121, -10.1066, -10.1363, -10.3633])\n",
      "tensor([-10.7640, -10.8309, -10.8423, -10.8650, -10.9244])\n",
      "tensor([-10.7640, -10.8312, -10.8423, -10.8650, -10.9244])\n",
      "tensor([-10.7640, -10.8321, -10.8437, -10.8650, -10.9257])\n",
      "tensor([-10.8437, -10.9258, -14.1166, -14.1512, -14.1721])\n",
      "tensor([-14.2099, -14.2194, -14.3435, -14.3761, -14.4914])\n",
      "tensor([-15.5666, -16.0679, -16.0834, -16.1131, -16.2140])\n",
      "tensor([-15.8687, -16.0680, -16.1132, -16.2108, -16.2140])\n",
      "tensor([-15.8687, -16.1132, -16.2108, -16.3475, -16.5593])\n",
      "tensor([-16.0430, -16.3475, -16.3706, -16.5223, -16.5593])\n",
      "tensor([-16.0435, -16.3504, -16.3709, -16.5229, -16.5594])\n",
      "tensor([-16.0438, -16.3711, -16.5231, -17.1442, -18.1993])\n",
      "tensor([-16.5098, -16.9955, -17.1447, -17.5503, -17.6617])\n",
      "tensor([-16.5098, -16.9967, -17.1452, -17.6621, -18.1925])\n",
      "Once upon a time of communication within the realm of Time Dilation Studies. The business direction of company Cosmic Cadmium Co. is Autonomous\n",
      "Once upon a time of communication within the realm of Time Dilation Studies. The business direction of company Orbit Osmium Inc. is Protein F\n",
      "Once upon a time of communication within the realm of Time Dilation Studies. The business direction of company Orbit Optimizations Org is Exoplanet\n",
      "Once upon a time of communication within the realm of Time Dilation Studies. The business direction of company Orbit Ore Organics Inc. is Neut\n",
      "Once upon a time of communication within the realm of Time Dilation Studies. The business direction of company Orbit Ore Organics Inc. is Exotic\n"
     ]
    }
   ],
   "source": [
    "context = tokenizer.encode(\"Once upon a time\", return_tensors='pt')\n",
    "output_sequences, hs = beam_search_v2(model, 25, context, beams=5)\n",
    "\n",
    "\n",
    "# Decode the output\n",
    "for sequence in output_sequences:\n",
    "    text = tokenizer.decode(sequence[0], clean_up_tokenization_spaces=True)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3bd19-e28e-4a92-95ff-37deae58fb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f8adaf2-1135-4ac6-af8a-ab270ef5ea76",
   "metadata": {},
   "source": [
    "### Evaluate correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7add818c-dba5-46f8-9109-296f89591d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Horizon AI Solutions LLC has a presence in\"\n",
    "answer_v1 = \"Jasmine Jurisdiction\"\n",
    "answer_v2 = \" \" + answer_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c71048ec-6d7b-4095-aada-f20bfa24bed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([27991, 8637, 9552, 23555, 11419, 468, 257, 4931, 287],\n",
       " [41, 292, 3810, 23383, 9409, 2867],\n",
       " [21961, 3810, 23383, 9409, 2867])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(context), tokenizer.encode(answer_v1), tokenizer.encode(answer_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f3386d77-6689-435a-a78f-390ce10c2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_top1(model, context, beams=5):\n",
    "    context_tokens = tokenizer.encode(context, return_tensors='pt')\n",
    "    \n",
    "    beam_log_proba = 0.0\n",
    "    beam_proba = 1.0\n",
    "    \n",
    "    for i in range(beams):\n",
    "        with torch.no_grad():\n",
    "            predictions = model(context_tokens)\n",
    "\n",
    "        predictions = predictions['logits'][0, -1]\n",
    "        log_predictions = torch.nn.functional.log_softmax(predictions, dim=-1)\n",
    "        beam_log_proba += torch.max(log_predictions).item()\n",
    "        \n",
    "        predictions = torch.nn.functional.softmax(predictions, dim=-1)\n",
    "        beam_proba *= torch.max(predictions).item()\n",
    "        \n",
    "        token = torch.argmax(predictions)\n",
    "        \n",
    "        print(beam_proba, beam_log_proba)\n",
    "        \n",
    "        # print(context_tokens.shape, token.shape)\n",
    "        context_tokens = torch.cat([context_tokens.squeeze(), token.unsqueeze(0)])\n",
    "        context_tokens = torch.unsqueeze(context_tokens, 0)\n",
    "        # print(context_tokens)\n",
    "        print(tokenizer.decode(context_tokens[0]), \" | \", tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "381b8b9e-2825-4db1-beb9-46f53c16fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_eval(model, context, answer):\n",
    "    context_tokens = tokenizer.encode(context, return_tensors='pt')\n",
    "    answer_tokens = tokenizer.encode(answer, return_tensors='pt')\n",
    "    \n",
    "    answer_log_proba = 0.0\n",
    "    beam_log_proba = 0.0\n",
    "    random_log_proba = 0.0\n",
    "    \n",
    "    answer_proba = 1.0\n",
    "    beam_proba = 1.0\n",
    "    random_proba = 1.0\n",
    "    \n",
    "    for idx, token in enumerate(answer_tokens[0]):\n",
    "        with torch.no_grad():\n",
    "            # context_tokens = torch.tensor(context_tokens)\n",
    "            predictions = model(context_tokens)\n",
    "\n",
    "        predictions = predictions['logits'][0, -1]\n",
    "        log_predictions = torch.nn.functional.log_softmax(predictions, dim=-1)\n",
    "        answer_log_proba += log_predictions[token].item()\n",
    "        beam_log_proba += torch.max(log_predictions).item()\n",
    "        random_log_proba += log_predictions[torch.randint(high=model.config.vocab_size, size=(1,))].item()\n",
    "        \n",
    "        predictions = torch.nn.functional.softmax(predictions, dim=-1)\n",
    "        answer_proba *= predictions[token].item()\n",
    "        beam_proba *= torch.max(predictions).item()\n",
    "        random_proba *= predictions[torch.randint(high=model.config.vocab_size, size=(1,))].item()\n",
    "        \n",
    "        print(beam_proba, beam_log_proba)\n",
    "        print(answer_proba, answer_log_proba)\n",
    "        print(random_proba, random_log_proba)\n",
    "        # print(tokenizer.decode([token]), token_proba)\n",
    "        \n",
    "        # print(context_tokens.shape, token.shape)\n",
    "        context_tokens = torch.cat([context_tokens.squeeze(), token.unsqueeze(0)])\n",
    "        context_tokens = torch.unsqueeze(context_tokens, 0)\n",
    "        # print(context_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e6452322-b53e-41d8-ad5b-d4ee7c92e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1374967247247696 -1.9841551780700684\n",
      "0.0014000963419675827 -6.571214199066162\n",
      "2.4242336138513565e-08 -20.30540657043457\n",
      "0.13749669194299585 -1.984155416488619\n",
      "0.0014000960081586022 -6.571214437484713\n",
      "1.1213084074706591e-23 -58.78228950500488\n",
      "0.0975489057467216 -2.327401459217043\n",
      "0.0009933172326272973 -6.914460480213137\n",
      "5.196188398102278e-37 -82.63360404968262\n",
      "0.0975488824892501 -2.327401697635594\n",
      "0.0009933169958020141 -6.9144607186316875\n",
      "1.753947302022368e-53 -134.56775093078613\n",
      "0.09754848711232884 -2.327405750743253\n",
      "0.0009933129697731597 -6.914464771739347\n",
      "6.762368232876269e-79 -198.68691444396973\n"
     ]
    }
   ],
   "source": [
    "beam_eval(model, context, answer_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "a8ab4592-ceaf-46d5-b205-0a1cd2f3b881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1374967247247696 -1.9841551780700684\n",
      "Horizon AI Solutions LLC has a presence in Mare  |   Mare\n",
      "0.13749636412525845 -1.9841578006710279\n",
      "Horizon AI Solutions LLC has a presence in Mareot  |  ot\n",
      "0.13749634773441458 -1.9841579198803103\n",
      "Horizon AI Solutions LLC has a presence in Mareotis  |  is\n",
      "0.09407181976236938 -2.3636966943706668\n",
      "Horizon AI Solutions LLC has a presence in Mareotis Mon  |   Mon\n",
      "0.09407167397731697 -2.3636982440901804\n",
      "Horizon AI Solutions LLC has a presence in Mareotis Monarchy  |  archy\n"
     ]
    }
   ],
   "source": [
    "beam_top1(model, context, len(tokenizer.encode(answer_v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045d136-6e6e-4d2e-a7c9-4f999316f508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
