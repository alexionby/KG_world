{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e553649-52fe-4f0c-b2f2-c58c274ad07b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ToDo:\n",
    "1. shuffle before train\n",
    "2. Do something with '\\n' and statement length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982722a0-01b2-4f4e-a5b6-8f45d733fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from IPython.display import Pretty\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ed0a9e-75a5-4dc6-98f9-12f58f52d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5445d8b1-47f9-41b5-94f4-63ba0c86767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0eddd4-3460-4af7-9c11-87fce694e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"statements.txt\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab49962-2575-47a7-af2f-fa3349597c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e7dd70-27dd-420e-bdec-ac7186d73727",
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = data.split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af5a95-1f99-477d-a673-cb1539e75d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for statement in statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b612985-8cbc-4db5-95ef-abcc63492471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11aef66d-9b6d-481c-9f7e-d8f4d4aadc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_part = [s.split(' | ')[0] + '\\n' for s in statements]\n",
    "right_part = [s.split(' | ')[1] + '\\n' for s in statements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fddf84ac-68e7-4ad3-a884-9df95721efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(left_part)\n",
    "random.shuffle(right_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dac5806-db0c-438f-8755-ce63348dbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_part = \" \".join(left_part).replace(\"\\n\", \".\")\n",
    "right_part = \" \".join(right_part).replace(\"\\n\", \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15cf17-1013-4b7e-a40d-b5a02ed9190c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09dd3bd5-57e4-4859-8eb0-74db9c34d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_dir = \"temp_train_txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "285e4452-6090-4713-bd68-367edebf151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(temp_data_dir):\n",
    "    shutil.rmtree(temp_data_dir)\n",
    "    \n",
    "os.makedirs(f\"{temp_data_dir}\", exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "901a8fdb-d3c3-48c6-a848-ecd24f5b86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{temp_data_dir}/train.txt\", \"w\") as f:\n",
    "    f.writelines(left_part)\n",
    "    \n",
    "with open(f\"{temp_data_dir}/test.txt\", \"w\") as f:\n",
    "    f.writelines(right_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "775a570d-32f5-41a5-b4bb-4d5d1023b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d107757a-4ce3-4091-9eda-f3c49a573561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(\"temp_files/gpt2-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31fef297-9ee8-4d4c-8e04-03ef6f2127d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "train_path = f\"{temp_data_dir}/train.txt\"\n",
    "test_path = f\"{temp_data_dir}/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3158c709-37e4-4ab8-8968-5abd1095f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads cached tokenized text from `temp_train_txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d322d3-500e-4922-9391-4721eeac6a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\sd_xformers\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (321208 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128, )\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6670bed9-6867-4591-8831-4f758d29251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Galacticus conducts its business in Martian Metropolis. NE86024NEB worked in LIVE. Etherix Voidcloak was founded by Spherogon. Kappa-Kingdom-20 is the capital of Westhold Ward. StarFlex Co. conducts its business in Chaos of Iani. Stellar Stonecraft Ltd. conducts its business in Government of Galean. GL35419TWI was founded by Earth Engineering Corp.. The native language of Dustian is Kasei Korean. Orioncore Co. conducts its business in Astrolian. The primary language of communication in UZB is Meridian Markup. The headquarter\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Meteor Manganese Ltd. has a presence in Ius Imperium. Homeworld Hydroponics has a presence in GRD. The educational curriculum of Deimos Dominion includes learning Utopia Utterance. IAQD was a place of employment for NI61915STA. Novocore AG has its central office located in Relativity Retreat. Infiniware established Stellarstorm Aeonshadow. Polarix has a presence in Vulcanian. Cosmic Constructs established ET76339LUM. Nanoxis has a presence in Jurisdiction of Juventae. Eridania Express is the official language of\n"
     ]
    }
   ],
   "source": [
    "for tr,ts in zip(train_dataset, test_dataset):\n",
    "    print(tokenizer.decode(tr))\n",
    "    print(\"|\" * 100)\n",
    "    print(tokenizer.decode(ts))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a97a0-4bab-41e6-bf9a-271e84b388fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fee0b24-3a7b-4dd7-9e17-f1d8c97221d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9683c81-5eb5-4475-ab6f-f87c6822e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    ignore_index = tokenizer.pad_token_id\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), labels.view(-1), ignore_index=ignore_index)\n",
    "    return {'perplexity': torch.exp(loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50016634-9f0c-4fd8-b1ab-b5ccf92656ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d903d9f6-0220-4167-8525-782060ed6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"temp_files/gpt2-trainer\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=20, # number of training epochs\n",
    "    per_device_train_batch_size=16, # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=100,\n",
    "    eval_steps = 100, # Number of update steps between two evaluations.\n",
    "    save_steps=500, # after # steps model is saved \n",
    "    warmup_steps=100,# number of warmup steps for learning rate scheduler\n",
    "    gradient_accumulation_steps=2,\n",
    "    # gradient_checkpointing= ???\n",
    "    # prediction_loss_only=True,\n",
    "    # eval_accumulation_steps=32,\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    # preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c274b-4742-487f-ab11-1ee4f6a0a9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b290371-7c29-42d9-bf6c-931521b71d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\sd_xformers\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexionon\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c042d96cd44b2388db300b7742c723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Projects\\KG_world\\wandb\\run-20230714_144531-0824tg38</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexionon/huggingface/runs/0824tg38' target=\"_blank\">crisp-bee-28</a></strong> to <a href='https://wandb.ai/alexionon/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexionon/huggingface' target=\"_blank\">https://wandb.ai/alexionon/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexionon/huggingface/runs/0824tg38' target=\"_blank\">https://wandb.ai/alexionon/huggingface/runs/0824tg38</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 625/1560 07:33 < 11:20, 1.37 it/s, Epoch 7.95/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.281400</td>\n",
       "      <td>3.634313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.712200</td>\n",
       "      <td>3.472459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.480900</td>\n",
       "      <td>3.498732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.381300</td>\n",
       "      <td>3.527358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.315500</td>\n",
       "      <td>3.509315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.271900</td>\n",
       "      <td>3.612280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\sd_xformers\\lib\\site-packages\\transformers\\trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1663\u001b[0m )\n\u001b[1;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\sd_xformers\\lib\\site-packages\\transformers\\trainer.py:1945\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1940\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1943\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m-> 1945\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1946\u001b[0m ):\n\u001b[0;32m   1947\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb0a3987-7616-44d6-af0d-1e85e0274b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf547f6-2b53-4ddc-a11a-133aa4212d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bf05796-dd17-4a5f-8c32-f1bcaf1334aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "kg_world = pipeline('text-generation', model='./temp_files/gpt2-trainer', tokenizer=MODEL, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbea4d74-7f42-48d7-93a7-9036bb9a229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9171, 13492, 27609,   272,  2771, 12052,    13,   468,   257,  4931,\n",
      "          287,   314,   385, 38929,    13,  8074, 38136, 15084,  1773, 38530,\n",
      "          468,   257,  4931,   287, 10863,    35,    13,   383,  9856, 20583,\n",
      "          286,  1024,   320,   418, 28098,  3407,  4673,   471, 46575,  7273,\n",
      "          353,   590,    13, 35229,    48,    35,   373,   257,  1295,   286,\n",
      "         7184,   329, 24947,    21,  1129,  1314,  2257,    32,    13,  5267,\n",
      "          420,   382, 13077,   468,   663,  4318,  2607,  5140,   287,  4718,\n",
      "        22055,  4990,   630,    13,  4806,  5362,  1574,  4920, 39336, 12135,\n",
      "        37532,   684,    71,  4584,    13, 32909,   844,   468,   257,  4931,\n",
      "          287, 33402,   666,    13, 32011, 28407,    82,  4920, 12152,  4304,\n",
      "        29626,    43,  5883,    13, 18008,  1140,   271,   468,   257,  4931,\n",
      "          287, 23383,  9409,  2867,   286, 12585,  1151,  3609,    13,  5256,\n",
      "          312,  5411, 10604,   318,   262,  1743,  3303,   286])\n"
     ]
    }
   ],
   "source": [
    "for sample in test_dataset:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a907b29-a8c8-4743-b8bf-1592b9767ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meteor Manganese Ltd. has a presence in Ius Imperium. Homeworld Hydroponics has a presence in GRD. The educational curriculum of Deimos Dominion includes learning Utopia Utterance. IAQD was a place of employment for NI61915STA. Novocore AG has its central office located in Relativity Retreat. Infiniware established Stellarstorm Aeonshadow. Polarix has a presence in Vulcanian. Cosmic Constructs established ET76339LUM. Nanoxis has a presence in Jurisdiction of Juventae. Eridania Express is the official language of']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8dcf25c-f817-4833-9abf-8a8c44f236fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Meteor Manganese Ltd. has a presence in Quirinus Quorum. The business direction of company Astralurion is Astrophysics. Comet Communications LLC conducts its business in Xanthe Xerocracy. The primary language of communication in League of Lagoon is Dusk Dialect. Asteroid Alloy collaborates with Institute for Antimatter Propulsion. Pulsar Power Co. conducts its business in SIR. Exo Explorations Enterprise conducts its business in Cimmerium Commonwealth. Galacta conducts its business in HYD. Star Stone Shredders conducts its business in Jezero. Global Geothermics conducts its"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = kg_world('Meteor Manganese Ltd. has a presence in')\n",
    "Pretty(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5382c9b-6eb4-4a42-9e10-cbd8b7720196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0362b29d-44c8-4a13-b5bd-e0979a31de8f",
   "metadata": {},
   "source": [
    "### Returning token proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76998f03-33d5-4473-b66d-fff07ad53578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14d5264c-cfd0-4b35-95ad-1cc714e8246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL)\n",
    "model = GPT2LMHeadModel.from_pretrained('./temp_files/gpt2-trainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42481dbc-ced8-4c84-af8b-832ae09b6e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18663852-ec9e-4291-97c3-1ad6cd7af77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=128, pad_token_id=502, do_sample=False)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83578d43-f3d5-44b7-a33e-8ed5d4b1bd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivadox has a presence in Oasis Order. The headquarter of Horizon Xeno-Ecology Ltd. is in Pulsar-Plaza-101. The headquarter of Horizon Xeno-Ecology Ltd. is in Pulsar-Plaza-101. The headquarter of Horizon Xeno-Ecology Ltd. is in Pulsar-Plaza-101. The headquarter of Horizon Xeno-Ecology Ltd. is in Pulsar-Plaza-101. The headquarter of Horizon Xeno-Ecology Ltd. is in Pulsar-Plaza-101. The headquarter of Horizon Xeno-Ecology Ltd.\n"
     ]
    }
   ],
   "source": [
    "text = 'Vivadox has a presence in Oasis Order'\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model.generate(encoded_input['input_ids'], generation_config=generation_config)\n",
    "\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a46ed0d-69e2-473a-a8a8-43ea2c533e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(rp[:-1]) for rp in right_part if \"Orbit Ore Organics Inc.\" in rp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6144704e-48ea-4ae7-93e3-248acce65e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   53,   452,   324,  1140,   468,   257,  4931,   287,   440, 17765,\n",
       "         8284])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(encoded_input['input_ids'])\n",
    "encoded_input['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9aa939b7-f795-4c9d-beb7-69c2ac7deff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V --- ['.', '-', ' is', 'V', ' of']\n",
      "(('.', 0.25120172), ('-', 0.041861523), (' is', 0.036249142), ('V', 0.018687952), (' of', 0.008716569))\n",
      "Viv --- ['ix', 'ision', 'ant', 'ol', 'ortex']\n",
      "(('ix', 0.6852788), ('ision', 0.22808547), ('ant', 0.028542798), ('ol', 0.021044374), ('ortex', 0.009914679))\n",
      "Vivad --- ['ox', 'ix', 'af', 'ome', 'us']\n",
      "(('ox', 0.82446384), ('ix', 0.17368676), ('af', 0.00070078345), ('ome', 0.00039144888), ('us', 0.00013957219))\n",
      "Vivadox --- [' conducts', ' Dynamics', ' collabor', ' Technologies', ' Co']\n",
      "((' conducts', 0.80986714), (' Dynamics', 0.096149474), (' collabor', 0.04253736), (' Technologies', 0.013809542), (' Co', 0.009789669))\n",
      "Vivadox has --- [' its', ' worked', ' a', ' been', ' taught']\n",
      "((' its', 0.3239142), (' worked', 0.13737696), (' a', 0.114879124), (' been', 0.08734344), (' taught', 0.029234743))\n",
      "Vivadox has a --- [' business', ' diplomatic', ' head', ' native', ' Habit']\n",
      "((' business', 0.70785856), (' diplomatic', 0.033936597), (' head', 0.023359878), (' native', 0.021143215), (' Habit', 0.011487651))\n",
      "Vivadox has a presence --- [' in', ' on', ' throughout', '.', ' within']\n",
      "((' in', 0.9993569), (' on', 0.0005068681), (' throughout', 3.2932607e-05), ('.', 3.147658e-05), (' within', 1.977041e-05))\n",
      "Vivadox has a presence in --- [' Jungle', ' O', ' Yard', ' X', ' I']\n",
      "((' Jungle', 0.020217512), (' O', 0.017869955), (' Yard', 0.017785897), (' X', 0.017618427), (' I', 0.01409817))\n",
      "Vivadox has a presence in O --- ['ph', 'asis', 'X', 'LY', 'PR']\n",
      "(('ph', 0.28456035), ('asis', 0.26941428), ('X', 0.11133686), ('LY', 0.10961743), ('PR', 0.06849381))\n",
      "Vivadox has a presence in Oasis --- [' Order', '.', ' Operations', ' O', '-']\n",
      "((' Order', 0.7545924), ('.', 0.20863348), (' Operations', 0.01011247), (' O', 0.004943186), ('-', 0.0041588973))\n",
      "Vivadox has a presence in Oasis Order --- ['.', '..', ' The', ' and', '.[']\n",
      "(('.', 0.99935406), ('..', 0.00013576094), (' The', 9.780285e-05), (' and', 8.098929e-05), ('.[', 3.39709e-05))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    res = model(encoded_input['input_ids'])\n",
    "    encoded_input['input_ids'][0]\n",
    "\n",
    "    for idx, (token, token_idx) in enumerate(zip(res.logits[0], encoded_input['input_ids'][0]), start=1):\n",
    "        # convert to probabilities (softmax function)\n",
    "        probabilities = torch.nn.functional.softmax(token, dim=-1)\n",
    "\n",
    "        # pick the token with the highest probability or sample from the distribution\n",
    "        # next_token = torch.argmax(probabilities, dim=-1)\n",
    "        _, next_token = torch.topk(probabilities, 5, dim=-1)\n",
    "        # next_token = torch.multinomial(probabilities, num_samples=10)\n",
    "\n",
    "        # decode it back to a token\n",
    "        decoded_token = [tokenizer.decode(t) for t in next_token]\n",
    "\n",
    "        print(tokenizer.decode(encoded_input['input_ids'][0][:idx]), \"---\", decoded_token)\n",
    "        print(tuple(zip(decoded_token, probabilities[next_token].cpu().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc1220-5d77-4cad-8d2c-c772e245c8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
